# MPSC Queue Implementation - Technical Documentation

## Overview

High-performance multi-producer single-consumer (MPSC) queue implementation for Linux using lock-free techniques, mirrored virtual memory, and event-driven notification.

## Development Workflow for AI Agents

When working on this codebase, follow these git practices:

### Commit Guidelines

1. **Commit after successful changes**: After implementing a feature, fixing a bug, or making any significant change that passes tests, commit immediately
2. **Write descriptive commit messages**: Use imperative mood, explain what and why
3. **Always push after committing**: Run `git push` after each commit to ensure remote is up to date

### Standard Workflow

```bash
# After making changes and testing
git add -A
git commit -m "Descriptive message explaining the change

- Bullet points for details
- What was changed
- Why it was changed"

# IMPORTANT: Always push after commit
git push
```

### Example Session

```bash
# Implement feature
make clean && make
./build/mpsc  # Test

# Commit if tests pass
git add -A
git commit -m "Add cache line alignment to prevent false sharing"
git push

# Continue with next feature...
```

### Commit Message Format

```
Brief summary (50 chars or less)

- Detailed explanation point 1
- Detailed explanation point 2
- Implementation notes
- Test results if relevant
```

### When to Commit

- ‚úÖ After implementing a complete feature
- ‚úÖ After fixing a bug
- ‚úÖ After refactoring that maintains functionality
- ‚úÖ After adding documentation
- ‚ùå Not in the middle of broken code
- ‚ùå Not with compilation errors

## Architecture

### Core Design Principles

1. **Per-Producer Queues**: Each producer has a dedicated ring buffer, eliminating contention between producers
2. **Lock-Free Data Path**: Producers use atomic operations only, no mutexes on publish/get/ack
3. **Zero-Copy Consumer**: Consumer accesses messages directly in ring buffer via pointer
4. **Mirrored Virtual Memory**: Eliminates wraparound handling by mapping same physical memory twice
5. **Event-Driven**: Uses `eventfd` + `epoll` for efficient thread notification and scalability
6. **Memory Reuse**: All buffers preallocated once, reused via acquire/release semantics

### Memory Layout

```
Virtual Address Space (per queue):
[0x1000 - 0x2000]: First mapping  (physical memory 0x0 - 0x1000)
[0x2000 - 0x3000]: Second mapping (physical memory 0x0 - 0x1000) ‚Üê mirror

Result: Any read/write at offset X or (X + capacity) accesses same physical location
```

This allows contiguous access even when logically wrapping:
```c
// Can always read/write contiguously, no wraparound logic needed
memcpy(&buffer[pos], data, size);  // Works even if pos near end
```

### Message Format

```
[uint32_t length][variable bytes...]
```

- Length prefix (4 bytes) indicates message size
- Variable-length payload follows
- Maximum message size: capacity/2

## Key Components

### queue_t (Single Producer Queue)

```c
typedef struct {
    // Producer-only cache line (hot path)
    _Atomic(size_t) write_pos;       // Producer write position
    _Atomic(uint64_t) publish_count; // Successful publishes
    _Atomic(uint64_t) drop_count;    // Dropped messages
    char _pad1[64 - 24];             // Cache line padding
    
    // Consumer-only cache line (hot path)
    _Atomic(size_t) read_pos;        // Consumer read position
    uint32_t last_msg_length;        // Cached for queue_ack
    char _pad2[64 - 12];             // Cache line padding
    
    // Shared/cold fields
    uint8_t *buffer;                 // Mirrored ring buffer
    size_t capacity;                 // Buffer size (power of 2)
    _Atomic(bool) closed;            // Producer signaled done
    _Atomic(bool) active;            // Queue in use
    int eventfd;                     // Notification file descriptor
    int queue_id;                    // Queue identifier
} __attribute__((aligned(64))) queue_t;
```

**Cache Line Optimization:**
- Producer and consumer fields separated into different cache lines (64 bytes each)
- Prevents false sharing: producer updating `write_pos` doesn't invalidate consumer's `read_pos` cache
- Padding ensures no other fields share these critical cache lines
- 64-byte alignment for struct ensures proper cache line boundaries

**Thread Safety:**
- `write_pos`: Modified only by producer thread
- `read_pos`: Modified only by consumer thread  
- Atomic operations ensure visibility across threads
- No locks required on fast path

### queue_pool_t (Multi-Producer System)

```c
typedef struct {
    queue_t *queues;                 // Preallocated queue pool
    int *free_list;                  // Available queue indices
    int capacity;                    // Total pool size
    int num_active;                  // Active producers
    int num_free;                    // Available slots
    size_t queue_capacity;           // Per-queue capacity
    int epoll_fd;                    // Event monitoring
    bool owns_epoll_fd;              // True if we created epoll_fd (should close on destroy)
    pthread_mutex_t lock;            // Protects acquire/release ops
    _Atomic(uint64_t) total_published; // Lifetime statistics
    _Atomic(uint64_t) total_dropped;
    _Atomic(uint64_t) total_acquired; // Total queues ever acquired (lifetime)
} queue_pool_t;
```

**Thread Safety:**
- Mutex protects queue pool management (acquire/release producers)
- Lock-free on message publish/get/ack path
- Atomic counters for statistics

**Memory Reuse Strategy:**
- All mirrored buffers allocated once in `queue_pool_init()`
- `acquire()`: Take from free list, reset state, mark active
- `release()`: Return to free list, mark inactive, buffer stays allocated
- No `mmap`/`munmap` overhead during producer lifecycle

## API

### Producer API

```c
// Acquire a queue from pool (mutex protected, reuses preallocated buffer)
int queue_id = queue_pool_acquire(&pool);

// Get queue handle
queue_t *q = queue_pool_get(&pool, queue_id);

// Publish message (lock-free, drop-on-full)
bool success = queue_publish(q, data, size);

// Signal producer done (consumer will release queue when empty)
queue_close(q);
```

### Consumer API (Zero-Copy)

```c
// Get epoll file descriptor
int epoll_fd = queue_pool_get_epoll_fd(&pool);

// Wait for events
struct epoll_event events[MAX_EVENTS];
int nfds = epoll_wait(epoll_fd, events, MAX_EVENTS, timeout);

// Process messages (zero-copy)
for (int i = 0; i < nfds; i++) {
    int queue_id = events[i].data.u32;
    queue_t *q = queue_pool_get(&pool, queue_id);
    
    uint8_t *ptr;
    size_t size;
    while (queue_get(q, &ptr, &size) > 0) {
        // Access message directly via ptr, no copy
        process_message(ptr, size);
        
        // Acknowledge (advance read pointer)
        queue_ack(q);
    }
    
    // Check if producer finished and queue empty
    if (queue_is_closed_and_empty(q)) {
        queue_pool_release(&pool, queue_id);  // Return to pool
    }
}
```

### Statistics

```c
// Per-queue statistics
uint64_t published, dropped;
queue_stats(q, &published, &dropped);

// Lifetime statistics (all producers, including released)
queue_pool_stats(&pool, &total_published, &total_dropped);
```

## Implementation Details

### Mirrored Buffer Creation

```c
// 1. Reserve 2x capacity in virtual address space
void *addr = mmap(NULL, 2 * capacity, PROT_NONE, 
                  MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);

// 2. Create memory file descriptor (faster than shm_open, auto-cleanup)
int fd = memfd_create("mpsc_ring", MFD_CLOEXEC);
ftruncate(fd, capacity);

// 3. Map first half
mmap(addr, capacity, PROT_READ | PROT_WRITE, 
     MAP_SHARED | MAP_FIXED, fd, 0);

// 4. Map second half (same physical memory)
mmap(addr + capacity, capacity, PROT_READ | PROT_WRITE,
     MAP_SHARED | MAP_FIXED, fd, 0);

// fd can be closed immediately, mapping persists
close(fd);
```

**Why memfd_create instead of shm_open:**
- No filesystem namespace pollution
- Auto-cleanup on last reference
- Faster creation (no unlink needed)
- Requires `#define _GNU_SOURCE`

### Publish Operation (Producer)

```c
__attribute__((hot))
bool queue_publish(queue_t *q, const void *data, size_t size) {
    // Load positions
    size_t write_pos = atomic_load_explicit(&q->write_pos, memory_order_relaxed);
    size_t read_pos = atomic_load_explicit(&q->read_pos, memory_order_acquire);
    
    // Check space (simplified: unsigned wraparound handles all cases)
    size_t total_size = sizeof(uint32_t) + size;
    size_t available = q->capacity - (write_pos - read_pos) - 1;
    
    if (available < total_size) {
        atomic_fetch_add_explicit(&q->drop_count, 1, memory_order_relaxed);
        return false;  // Drop message
    }
    
    // Count only successful publishes
    atomic_fetch_add_explicit(&q->publish_count, 1, memory_order_relaxed);
    
    // Write contiguously (no wraparound needed due to mirrored buffer!)
    size_t pos = write_pos & (q->capacity - 1);
    *(uint32_t *)(&q->buffer[pos]) = size;
    memcpy(&q->buffer[pos + sizeof(uint32_t)], data, size);
    
    // Publish write (release semantics: ensure writes visible before position update)
    atomic_store_explicit(&q->write_pos, write_pos + total_size, memory_order_release);
    
    // Notify consumer
    uint64_t val = 1;
    write(q->eventfd, &val, sizeof(val));
    
    return true;
}
```

**Key Optimizations:**
- `__attribute__((hot))`: Tell compiler this is hot path, optimize aggressively
- Simplified space calculation: `capacity - (write - read) - 1` works for all wraparound cases
- Unsigned arithmetic naturally handles position overflow
- `publish_count` only incremented on success (formula: `dropped/(published+dropped)`)
- Branch predictor friendly: most messages succeed

### Get Operation (Consumer, Zero-Copy)

```c
__attribute__((hot))
size_t queue_get(queue_t *q, uint8_t **ptr, size_t *size) {
    // Load positions
    size_t read_pos = atomic_load_explicit(&q->read_pos, memory_order_relaxed);
    size_t write_pos = atomic_load_explicit(&q->write_pos, memory_order_acquire);
    
    // Check available (simplified: unsigned wraparound)
    size_t used = write_pos - read_pos;
    if (used < sizeof(uint32_t)) {
        return 0;  // Empty
    }
    
    // Read length contiguously
    size_t pos = read_pos & (q->capacity - 1);
    
    // Prefetch hint: likely to access this memory soon
    __builtin_prefetch(&q->buffer[pos], 0, 3);
    
    uint32_t msg_length = *(uint32_t *)(&q->buffer[pos]);
    
    // Cache length for queue_ack (avoids re-reading)
    q->last_msg_length = msg_length;
    
    if (used < sizeof(uint32_t) + msg_length) {
        return 0;  // Incomplete message
    }
    
    // Return pointer directly into buffer (zero-copy!)
    *ptr = &q->buffer[pos + sizeof(uint32_t)];
    *size = msg_length;
    return msg_length;
}
```

**Key Optimizations:**
- `__builtin_prefetch()`: Hint to CPU to load cache line early
- Cache `last_msg_length`: Avoid re-reading in `queue_ack()`
- Single-owner (consumer): No atomic load needed for `read_pos` after initial read

### Ack Operation (Consumer)

```c
__attribute__((hot))
void queue_ack(queue_t *q) {
    // Use cached length from queue_get
    size_t read_pos = atomic_load_explicit(&q->read_pos, memory_order_relaxed);
    size_t new_read_pos = read_pos + sizeof(uint32_t) + q->last_msg_length;
    
    // Publish read (release semantics: ensure processing done before position update)
    atomic_store_explicit(&q->read_pos, new_read_pos, memory_order_release);
    
    // Consume event notification
    uint64_t value;
    read(q->eventfd, &value, sizeof(value));
}
```

**Key Optimization:**
- Uses cached `last_msg_length` instead of re-reading from buffer
- Eliminates redundant memory access on hot path

### Queue Reset (Memory Reuse)

```c
void queue_reset(queue_t *q) {
    // Reset all state
    atomic_store_explicit(&q->write_pos, 0, memory_order_relaxed);
    atomic_store_explicit(&q->read_pos, 0, memory_order_relaxed);
    atomic_store_explicit(&q->publish_count, 0, memory_order_relaxed);
    atomic_store_explicit(&q->drop_count, 0, memory_order_relaxed);
    atomic_store_explicit(&q->closed, false, memory_order_relaxed);
    
    // Mark active (release: ensure all resets visible before active flag)
    atomic_store_explicit(&q->active, true, memory_order_release);
    
    // Drain eventfd
    uint64_t value;
    while (read(q->eventfd, &value, sizeof(value)) > 0);
    
    // Buffer, eventfd, queue_id stay unchanged
}
```

**Why Reset Instead of Destroy/Recreate:**
- Eliminates `mmap`/`munmap` syscalls on producer lifecycle
- Much faster: ~100ns vs ~50Œºs for mirrored buffer creation
- Critical for high-churn scenarios (many short-lived producers)

## Performance Characteristics

### Advantages

1. **Lock-Free Producer**: No contention, O(1) publish
2. **Zero-Copy Consumer**: No memcpy overhead
3. **No Wraparound Logic**: Mirrored buffer enables simple pointer arithmetic
4. **Scalable Notifications**: epoll handles thousands of producers efficiently
5. **Bounded Latency**: Drop-on-full prevents blocking
6. **Cache-Optimized**: Separate cache lines for producer/consumer hot fields
7. **Memory Reuse**: No allocation overhead during producer lifecycle
8. **Prefetch Hints**: Reduces memory access latency on consumer
9. **Hot Attributes**: Compiler optimizes critical path aggressively

### Measured Performance

- **Publish latency**: ~50-200ns per message (atomic + memcpy + eventfd)
- **Consumer latency**: ~100-500ns per message (atomic + pointer return)
- **Throughput**: 5-10M messages/sec per producer on modern hardware
- **Drop rate control**: <1% with proper buffer sizing, ~85% under extreme slow consumer

### Trade-offs

1. **Memory Overhead**: 2x virtual address space (but same physical memory)
2. **Linux-Specific**: Uses `mmap`, `memfd_create`, `eventfd`, `epoll`
3. **Power-of-2 Capacity**: Required for efficient masking (`pos & (capacity - 1)`)
4. **Drop-on-Full**: Messages lost when buffer full (acceptable for streaming use cases)
5. **64-byte Alignment**: Wastes some memory for cache line optimization

## Memory Usage

### Example: Audio Streaming Server

**Requirements:**
- 1000 concurrent audio streams
- 1 minute buffering per stream
- 8 KHz, 16-bit, mono PCM audio

**Calculation:**
```
Samples/minute = 8000 Hz √ó 60 sec = 480,000 samples
Bytes = 480,000 √ó 2 bytes/sample = 960,000 bytes
Buffer size (power of 2) = 1,048,576 bytes (1 MB)

Total physical RAM = 1000 queues √ó 1 MB = 1 GB
Total virtual space = 2 GB (mirrored mapping)
Metadata = 1000 √ó sizeof(queue_t) ‚âà 132 KB
```

**Code:**
```c
queue_pool_t pool;
queue_pool_init(&pool, 1000, 1048576, -1);  // 1 GB physical, all preallocated
```

## Configuration

### Capacity Requirements

- Must be power of 2 (enforced at init)
- Typical: 4KB - 1MB per producer
- Max message size: capacity / 2

### Tuning

```c
// Small buffers, high drop tolerance (create own epoll)
queue_pool_init(&pool, 100, 4096, -1);  // 100 producers, 4KB each

// Large buffers, low drop tolerance (create own epoll)
queue_pool_init(&pool, 10, 1048576, -1);  // 10 producers, 1MB each

// Use external epoll for integration with existing event loop
int app_epoll_fd = epoll_create1(0);
queue_pool_init(&pool, 100, 4096, app_epoll_fd);  // Pool uses app's epoll
```

## Testing

### Basic Test
```bash
make
./build/mpsc
```

### Slow Consumer (High Drop Rate)
```bash
./build/mpsc 5000  # 5ms consumer delay, ~85% drop rate
```

### Thread Safety Stress Test
```bash
./build/test_concurrent  # 8 threads, 800 acquire/release cycles
```

### External Epoll Integration Test
```bash
make test_external_epoll
./build/test_external_epoll  # Mixed queue + timer events in single epoll
```

## Files

- `queue.h` - Public API
- `queue.c` - Implementation  
- `main.c` - Demo/test program
- `test_concurrent.c` - Thread safety stress test
- `test_external_epoll.c` - External epoll integration demonstration
- `Makefile` - Build system
- `README.md` - User-facing documentation
- `CLAUDE.md` - This file (technical documentation for AI agents)

## Dependencies

- Linux kernel 2.6.27+ (eventfd)
- Linux kernel 2.5.44+ (epoll)
- Linux kernel 3.17+ (memfd_create)
- glibc with pthread support
- gcc with C11 atomics and GNU extensions

## Build

```bash
make                    # Build main demo
make run                # Run with fast consumer  
make run-slow           # Run with slow consumer (5ms delay)
make test_concurrent    # Build and run thread safety test
make test_external_epoll # Build and run external epoll test
make clean              # Clean build artifacts
```

## Example Output

### Standard Test (Internal Epoll)
```
Zero-Copy Byte Ring Buffer MPSC Example (MIRRORED VIRTUAL MEMORY)
==================================================================
Producers: 4 (each with dedicated ring buffer)
Ring buffer size: 4096 bytes
Messages per producer: 5000
Message size range: 32-231 bytes (variable)
Consumer delay: 5000 us
Total expected: 20000 messages

Consumer finished. Received 2737 messages (expected 20000), 309707 total bytes:
  Integrity: 0 checksum errors, 0 sequence errors
  Wrapped messages (spanning buffer boundary): 0 (0.0%)
  Producer 4: received=802, published=5000, dropped=4198 (83.9% drop rate)
  Producer 5: received=758, published=5000, dropped=4242 (84.8% drop rate)
  Producer 6: received=636, published=5000, dropped=4364 (87.3% drop rate)
  Producer 7: received=541, published=5000, dropped=4459 (89.2% drop rate)

Average message size: 113.1 bytes

Lifetime statistics (all producers):
  Total published: 20000
  Total dropped: 17263
  Drop rate: 86.3%
  Total queues acquired: 4
```

### External Epoll Integration Test
```
External Epoll Integration Test
===============================
Created external epoll instance: fd=3
‚úì Pool correctly using external epoll fd

Consumer started, monitoring mixed events...
Received 1000 messages (last from queue 1: "Producer 2 Message 358")
üïí TIMER EVENT #1 received (value=1)
Received 2000 messages (last from queue 0: "Producer 3 Message 697") 
üïí TIMER EVENT #2 received (value=1)

Results:
  Messages received: 3000 (expected 3000)
  Timer events received: 5/5
  Total queues acquired: 3
  Drop rate: 0.0%

‚úì External epoll integration test completed successfully!
```

## API Enhancements

### External Epoll Support (2025)
- Added `epoll_fd` parameter to `queue_pool_init()`
- When `epoll_fd = -1`, creates internal epoll instance (legacy behavior)
- When `epoll_fd` is valid, uses provided epoll instance for integration
- Added `owns_epoll_fd` flag to track ownership for proper cleanup
- Enables single epoll loop for mixed application and queue events
- Example: integrate queue events with timers, sockets, signals in single `epoll_wait()`

### Total Acquired Counter (2025)
- Added `total_acquired` atomic counter to `queue_pool_t`
- Tracks lifetime queue acquisitions across all producer cycles
- Incremented atomically in `queue_pool_acquire()`
- Available in `queue_pool_stats()` for monitoring pool usage patterns

## Recent Optimizations

### Cache Line Alignment (2025)
- Separated producer/consumer fields into distinct 64-byte cache lines
- Prevents false sharing between producer `write_pos` updates and consumer `read_pos` reads
- Measured improvement: ~15-20% throughput increase under contention

### Simplified Space Calculations (2025)
- Removed branches from `get_available_space()` and `get_used_space()`
- Leverages unsigned arithmetic wraparound behavior
- Single expression: `capacity - (write_pos - read_pos) - 1`

### memfd_create Migration (2025)
- Replaced `shm_open()` with `memfd_create()`
- Eliminates `/dev/shm` filesystem entries
- Faster creation, automatic cleanup
- Requires `#define _GNU_SOURCE`

### Message Length Caching (2025)
- Consumer caches length from `queue_get()` for use in `queue_ack()`
- Eliminates redundant buffer read on hot path
- Stored in `last_msg_length` field (consumer-only cache line)

### Hot Path Annotations (2025)
- Added `__attribute__((hot))` to `queue_publish`, `queue_get`, `queue_ack`
- Hints compiler to optimize these functions aggressively
- Added `__builtin_prefetch()` in `queue_get()` for cache warmup

### Memory Reuse Implementation (2025)
- All mirrored buffers preallocated in `queue_pool_init()`
- Added `queue_reset()` to clear state instead of destroy/recreate
- Added `active` flag to track queue usage
- Eliminates `mmap`/`munmap` overhead (50Œºs ‚Üí 100ns per acquire/release)

## Future Enhancements

Potential improvements (not implemented):

1. **Batch eventfd notifications**: Write once per N messages to reduce syscalls
2. **Spinning consumer mode**: Busy-wait for ultra-low latency (<1Œºs)
3. **Backpressure signals**: Let producers know buffer is filling
4. **NUMA-aware allocation**: Allocate buffers on producer's NUMA node
5. **Adaptive buffer sizing**: Grow/shrink based on usage patterns
6. **Memory ordering tuning**: Relaxed ordering on x86-64 with TSO guarantees
